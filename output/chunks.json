[
  {
    "type": "function",
    "name": "check_file_exists",
    "file_path": "D:\\youtube-datalake\\dags\\youtube_monthly_etl.py",
    "start_line": 25,
    "end_line": 32,
    "file_role": "data pipeline orchestration",
    "class": null,
    "intent_tags": [],
    "control_flow": [
      "conditional_logic"
    ],
    "docstring": "Check if the YouTube watch history file exists before transformation.",
    "imports": {
      "modules": [
        "os"
      ],
      "symbols": {
        "DAG": "airflow",
        "PythonOperator": "airflow.operators.python",
        "datetime": "datetime",
        "transform_data": "scripts.transform_watch",
        "upload_file_to_s3": "scripts.upload_to_s3",
        "run_glue_crawler": "scripts.run_glue_crawler",
        "run_athena_queries": "scripts.run_athena_queries"
      }
    },
    "calls": [
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "os.path.exists",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "FileNotFoundError",
        "resolved_via": "local_or_unknown",
        "module": null
      }
    ],
    "code": "def check_file_exists(file_path: str):\n    \"\"\"Check if the YouTube watch history file exists before transformation.\"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(\n            f\" Missing input file: {file_path}\\n\"\n            \"Please download your YouTube watch-history.json and place it in /include/\"\n        )\n    print(f\" Found file: {file_path}\")\n",
    "retrieval_text": "FUNCTION NAME:\ncheck_file_exists\n\nFILE ROLE:\ndata pipeline orchestration\n\nLOCATION:\nD:\\youtube-datalake\\dags\\youtube_monthly_etl.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ngeneral_logic\n\nCONTROL FLOW:\nconditional_logic\n\nDOCSTRING SUMMARY:\nCheck if the YouTube watch history file exists before transformation.\n\nIMPORT CONTEXT:\nModules \u2192 os\nImported Symbols \u2192 DAG, PythonOperator, datetime, transform_data, upload_file_to_s3, run_glue_crawler, run_athena_queries\n\nFUNCTION CALLS:\nprint, os.path.exists, FileNotFoundError\n\nDESCRIPTION:\nThis function implements logic related to check_file_exists and is part of the\ndata pipeline orchestration. It may interact with other components via imported\nmodules and function calls listed above."
  },
  {
    "type": "function",
    "name": "check_file_exists",
    "file_path": "D:\\youtube-datalake\\dags\\scripts\\check_file.py",
    "start_line": 4,
    "end_line": 10,
    "file_role": "general application logic",
    "class": null,
    "intent_tags": [],
    "control_flow": [
      "conditional_logic"
    ],
    "docstring": null,
    "imports": {
      "modules": [
        "os"
      ],
      "symbols": {
        "AirflowSkipException": "airflow.exceptions"
      }
    },
    "calls": [
      {
        "name": "os.path.exists",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "AirflowSkipException",
        "resolved_via": "import",
        "module": "airflow.exceptions"
      }
    ],
    "code": "def check_file_exists():\n    filepath = \"/opt/airflow/dags/scripts/include/watch-history.json\"\n    if os.path.exists(filepath):\n        print(\"File exists, proceeding with task execution\")\n    else:\n        print(\"File not found, skipping the rest of the pipeline\")\n        raise AirflowSkipException(\"watch-history.json not found. Skipping DAG run.\")\n",
    "retrieval_text": "FUNCTION NAME:\ncheck_file_exists\n\nFILE ROLE:\ngeneral application logic\n\nLOCATION:\nD:\\youtube-datalake\\dags\\scripts\\check_file.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ngeneral_logic\n\nCONTROL FLOW:\nconditional_logic\n\nDOCSTRING SUMMARY:\nNo docstring provided.\n\nIMPORT CONTEXT:\nModules \u2192 os\nImported Symbols \u2192 AirflowSkipException\n\nFUNCTION CALLS:\nos.path.exists, print, print, AirflowSkipException\n\nDESCRIPTION:\nThis function implements logic related to check_file_exists and is part of the\ngeneral application logic. It may interact with other components via imported\nmodules and function calls listed above."
  },
  {
    "type": "function",
    "name": "run_athena_queries",
    "file_path": "D:\\youtube-datalake\\dags\\scripts\\run_athena_queries.py",
    "start_line": 5,
    "end_line": 48,
    "file_role": "general application logic",
    "class": null,
    "intent_tags": [
      "cloud_storage"
    ],
    "control_flow": [
      "looping",
      "conditional_logic"
    ],
    "docstring": null,
    "imports": {
      "modules": [
        "os",
        "boto3",
        "time"
      ],
      "symbols": {}
    },
    "calls": [
      {
        "name": "boto3.client",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "athena.start_query_execution",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "athena.get_query_execution",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "time.sleep",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      }
    ],
    "code": "def run_athena_queries():\n    athena = boto3.client(\n        \"athena\",\n        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=os.getenv(\"AWS_REGION\", \"ap-south-1\")\n    )\n\n    output_location = \"s3://nancy-youtube/athena-results/\"\n    database = \"youtube_db\"\n\n    queries = [\n        (\"Top Channels\",\n         \"SELECT channel_name, COUNT(*) AS videos_watched FROM youtube_curated \"\n         \"GROUP BY channel_name ORDER BY videos_watched DESC LIMIT 5;\"),\n\n        (\"Monthly Activity\",\n         \"SELECT year, month, COUNT(*) AS videos FROM youtube_curated \"\n         \"GROUP BY year, month ORDER BY year DESC, month DESC;\"),\n\n        (\"Most Active Days\",\n         \"SELECT day_of_week, COUNT(*) AS views FROM youtube_curated \"\n         \"GROUP BY day_of_week ORDER BY views DESC;\")\n    ]\n\n    for name, query in queries:\n        print(f\"\u25b6\ufe0f Running Athena Query: {name}\")\n        response = athena.start_query_execution(\n            QueryString=query,\n            QueryExecutionContext={\"Database\": database},\n            ResultConfiguration={\"OutputLocation\": output_location}\n        )\n\n        query_execution_id = response[\"QueryExecutionId\"]\n\n        # Wait for completion\n        while True:\n            result = athena.get_query_execution(QueryExecutionId=query_execution_id)\n            status = result[\"QueryExecution\"][\"Status\"][\"State\"]\n\n            if status in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n                print(f\"\u2705 Query '{name}' finished with status: {status}\")\n                break\n            time.sleep(5)\n",
    "retrieval_text": "FUNCTION NAME:\nrun_athena_queries\n\nFILE ROLE:\ngeneral application logic\n\nLOCATION:\nD:\\youtube-datalake\\dags\\scripts\\run_athena_queries.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ncloud_storage\n\nCONTROL FLOW:\nlooping, conditional_logic\n\nDOCSTRING SUMMARY:\nNo docstring provided.\n\nIMPORT CONTEXT:\nModules \u2192 os, boto3, time\nImported Symbols \u2192 \n\nFUNCTION CALLS:\nboto3.client, print, athena.start_query_execution, os.getenv, os.getenv, os.getenv, athena.get_query_execution, time.sleep, print\n\nDESCRIPTION:\nThis function implements logic related to run_athena_queries and is part of the\ngeneral application logic. It may interact with other components via imported\nmodules and function calls listed above."
  },
  {
    "type": "function",
    "name": "run_glue_crawler",
    "file_path": "D:\\youtube-datalake\\dags\\scripts\\run_glue_crawler.py",
    "start_line": 7,
    "end_line": 41,
    "file_role": "general application logic",
    "class": null,
    "intent_tags": [
      "cloud_storage"
    ],
    "control_flow": [
      "looping",
      "conditional_logic",
      "exception_handling"
    ],
    "docstring": null,
    "imports": {
      "modules": [
        "sys",
        "os",
        "boto3",
        "time"
      ],
      "symbols": {}
    },
    "calls": [
      {
        "name": "boto3.client",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "glue.start_crawler",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "time.time",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "glue.get_crawler",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "time.sleep",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "sys.exit",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "sys.exit",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "sys.exit",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "time.time",
        "resolved_via": "attribute",
        "module": null
      }
    ],
    "code": "def run_glue_crawler(crawler_name=\"youtube_crawler\" , timeout=60):\n    glue = boto3.client(\n        \"glue\",\n        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=os.getenv(\"AWS_REGION\", \"ap-south-1\")\n    )\n\n    print(f\"Starting Glue Crawler: {crawler_name}\")\n    glue.start_crawler(Name=crawler_name)\n\n    start_time = time.time()\n    while True:\n        try:\n            response = glue.get_crawler(Name=crawler_name)\n            status = response[\"Crawler\"][\"State\"]\n            print(f\"Crawler status: {status}\")\n\n            if status == \"READY\":\n                print(\"Glue Crawler completed successfully.\")\n                break\n\n            # Exit if timeout exceeded\n            if (time.time() - start_time) > timeout:\n                print(f\" Timeout: Crawler '{crawler_name}' did not complete within {timeout} seconds.\")\n                sys.exit(1)\n\n            time.sleep(10)\n\n        except glue.exceptions.EntityNotFoundException:\n            print(f\"Crawler '{crawler_name}' not found.\")\n            sys.exit(1)\n        except Exception as e:\n            print(f\"Error checking crawler status: {e}\")\n            sys.exit(1)\n",
    "retrieval_text": "FUNCTION NAME:\nrun_glue_crawler\n\nFILE ROLE:\ngeneral application logic\n\nLOCATION:\nD:\\youtube-datalake\\dags\\scripts\\run_glue_crawler.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ncloud_storage\n\nCONTROL FLOW:\nlooping, conditional_logic, exception_handling\n\nDOCSTRING SUMMARY:\nNo docstring provided.\n\nIMPORT CONTEXT:\nModules \u2192 sys, os, boto3, time\nImported Symbols \u2192 \n\nFUNCTION CALLS:\nboto3.client, print, glue.start_crawler, time.time, os.getenv, os.getenv, os.getenv, glue.get_crawler, print, time.sleep, print, print, sys.exit, print, sys.exit, print, sys.exit, time.time\n\nDESCRIPTION:\nThis function implements logic related to run_glue_crawler and is part of the\ngeneral application logic. It may interact with other components via imported\nmodules and function calls listed above."
  },
  {
    "type": "function",
    "name": "transform_data",
    "file_path": "D:\\youtube-datalake\\dags\\scripts\\transform_watch.py",
    "start_line": 5,
    "end_line": 53,
    "file_role": "general application logic",
    "class": null,
    "intent_tags": [],
    "control_flow": [
      "looping",
      "conditional_logic"
    ],
    "docstring": "Transforms the YouTube watch history JSON file into structured data.\nSaves processed files locally for later upload by separate DAG tasks.",
    "imports": {
      "modules": [
        "json",
        "pandas",
        "os"
      ],
      "symbols": {}
    },
    "calls": [
      {
        "name": "pd.DataFrame",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "pd.to_datetime",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "df['time'].dt.day_name",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "df.to_parquet",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "os.path.exists",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "FileNotFoundError",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "open",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "json.load",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "records.append",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get('subtitles', [{}])[0].get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get('subtitles', [{}])[0].get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "item.get",
        "resolved_via": "attribute",
        "module": null
      }
    ],
    "code": "def transform_data():\n    \"\"\"\n    Transforms the YouTube watch history JSON file into structured data.\n    Saves processed files locally for later upload by separate DAG tasks.\n    \"\"\"\n\n    input_path = \"/opt/airflow/dags/scripts/include/watch-history.json\"\n    staging_path = \"/opt/airflow/dags/scripts/data/processed_watch_history.csv\"\n    curated_parquet_path = \"/opt/airflow/dags/scripts/data/watch_history.parquet\"\n\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(\"watch-history.json not found!\")\n\n    with open(input_path, 'r') as file:\n        data = json.load(file)\n\n    # Normalize YouTube JSON\n    records = []\n    for item in data:\n        if \"titleUrl\" in item:\n            record = {\n                \"title\": item.get(\"title\"),\n                \"titleUrl\": item.get(\"titleUrl\"),\n                \"time\": item.get(\"time\"),\n                \"channel_name\": item.get(\"subtitles\", [{}])[0].get(\"name\"),\n                \"channel_url\": item.get(\"subtitles\", [{}])[0].get(\"url\"),\n            }\n            records.append(record)\n\n    df = pd.DataFrame(records)\n\n    # Add derived columns\n    df[\"time\"] = pd.to_datetime(df[\"time\"], errors='coerce')\n    df[\"year\"] = df[\"time\"].dt.year\n    df[\"month\"] = df[\"time\"].dt.month\n    df[\"day\"] = df[\"time\"].dt.day\n    df[\"weekday\"] = df[\"time\"].dt.day_name()\n    df[\"hour\"] = df[\"time\"].dt.hour\n\n    \n    # --- Save local staging CSV ---\n    # df.to_csv(staging_path, index=False)\n    print(f\"\u2705 Staging CSV saved \u2192 {staging_path}\")\n\n    # --- Save local curated Parquet ---\n    df.to_parquet(curated_parquet_path, index=False)\n    print(f\"\u2705 Curated Parquet saved \u2192 {curated_parquet_path}\")\n\n    print(\"\ud83c\udfaf Transformation complete. Files ready for upload.\")\n",
    "retrieval_text": "FUNCTION NAME:\ntransform_data\n\nFILE ROLE:\ngeneral application logic\n\nLOCATION:\nD:\\youtube-datalake\\dags\\scripts\\transform_watch.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ngeneral_logic\n\nCONTROL FLOW:\nlooping, conditional_logic\n\nDOCSTRING SUMMARY:\nTransforms the YouTube watch history JSON file into structured data.\nSaves processed files locally for later upload by separate DAG tasks.\n\nIMPORT CONTEXT:\nModules \u2192 json, pandas, os\nImported Symbols \u2192 \n\nFUNCTION CALLS:\npd.DataFrame, pd.to_datetime, df['time'].dt.day_name, print, df.to_parquet, print, print, os.path.exists, FileNotFoundError, open, json.load, records.append, item.get, item.get, item.get, item.get('subtitles', [{}])[0].get, item.get('subtitles', [{}])[0].get, item.get, item.get\n\nDESCRIPTION:\nThis function implements logic related to transform_data and is part of the\ngeneral application logic. It may interact with other components via imported\nmodules and function calls listed above."
  },
  {
    "type": "function",
    "name": "upload_file_to_s3",
    "file_path": "D:\\youtube-datalake\\dags\\scripts\\upload_to_s3.py",
    "start_line": 4,
    "end_line": 42,
    "file_role": "general application logic",
    "class": null,
    "intent_tags": [
      "data_persistence",
      "cloud_storage",
      "data_access"
    ],
    "control_flow": [
      "exception_handling",
      "conditional_logic"
    ],
    "docstring": "Upload a file to S3.\n\nArgs:\n    local_file (str): Local path of the file to upload.\n    s3_key (str): Key (path) to save in S3, e.g. 'raw/file.json' or 'staging/file.csv'.\n    bucket (str, optional): Target S3 bucket name. \n                            If not provided, defaults to AWS_S3_BUCKET from env.",
    "imports": {
      "modules": [
        "os",
        "boto3"
      ],
      "symbols": {}
    },
    "calls": [
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "boto3.client",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "os.getenv",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "ValueError",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "ValueError",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "s3.upload_file",
        "resolved_via": "attribute",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "print",
        "resolved_via": "local_or_unknown",
        "module": null
      },
      {
        "name": "str",
        "resolved_via": "local_or_unknown",
        "module": null
      }
    ],
    "code": "def upload_file_to_s3(local_file: str, s3_key: str, bucket: str = None):\n    \"\"\"\n    Upload a file to S3.\n\n    Args:\n        local_file (str): Local path of the file to upload.\n        s3_key (str): Key (path) to save in S3, e.g. 'raw/file.json' or 'staging/file.csv'.\n        bucket (str, optional): Target S3 bucket name. \n                                If not provided, defaults to AWS_S3_BUCKET from env.\n    \"\"\"\n\n    # Fallback to environment variable if bucket not passed\n    if bucket is None:\n        bucket = os.getenv(\"S3_BUCKET\")\n\n    if not bucket:\n        raise ValueError(\"S3 bucket not specified and AWS_S3_BUCKET not set in environment.\")\n\n    # Load credentials from environment (Docker will inject them)\n    aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n    aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n    aws_region = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n\n    if not aws_access_key_id or not aws_secret_access_key:\n        raise ValueError(\"AWS credentials not found in environment variables\")\n\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n        region_name=aws_region,\n    )\n\n    try:\n        s3.upload_file(local_file, bucket, s3_key)\n        print(f\" Uploaded {local_file} \u2192 s3://{bucket}/{s3_key}\")\n    except Exception as e:\n        print(f\" Error uploading {local_file} \u2192 s3://{bucket}/{s3_key}: {str(e)}\")\n        raise\n",
    "retrieval_text": "FUNCTION NAME:\nupload_file_to_s3\n\nFILE ROLE:\ngeneral application logic\n\nLOCATION:\nD:\\youtube-datalake\\dags\\scripts\\upload_to_s3.py\n\nCLASS CONTEXT:\nN/A\n\nINTENT TAGS:\ndata_persistence, cloud_storage, data_access\n\nCONTROL FLOW:\nexception_handling, conditional_logic\n\nDOCSTRING SUMMARY:\nUpload a file to S3.\n\nArgs:\n    local_file (str): Local path of the file to upload.\n    s3_key (str): Key (path) to save in S3, e.g. 'raw/file.json' or 'staging/file.csv'.\n    bucket (str, optional): Target S3 bucket name. \n                            If not provided, defaults to AWS_S3_BUCKET from env.\n\nIMPORT CONTEXT:\nModules \u2192 os, boto3\nImported Symbols \u2192 \n\nFUNCTION CALLS:\nos.getenv, os.getenv, os.getenv, boto3.client, os.getenv, ValueError, ValueError, s3.upload_file, print, print, str\n\nDESCRIPTION:\nThis function implements logic related to upload_file_to_s3 and is part of the\ngeneral application logic. It may interact with other components via imported\nmodules and function calls listed above."
  }
]